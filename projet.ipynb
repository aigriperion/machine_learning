{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "0d2f0453",
   "metadata": {},
   "outputs": [],
   "source": "# Prediction du prix des appartements — DVF\n\n**Objectif** : Predire le prix de vente (`valeur_fonciere`) des appartements en France a partir des donnees DVF (Demandes de Valeurs Foncieres).\n\n## 1. Chargement et exploration des donnees"
  },
  {
   "cell_type": "code",
   "id": "wxq28o679i",
   "source": "# Imports\nfrom pathlib import Path\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso\nfrom sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor\nfrom sklearn.inspection import permutation_importance\n\nsns.set_theme(style=\"whitegrid\")\npd.set_option(\"display.max_columns\", 50)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "0g4ecwwwkbsd",
   "source": "# Chargement de tous les fichiers CSV du dossier datasets/\ndata_dir = Path(\"datasets\")\ncsv_files = sorted(data_dir.glob(\"full_*.csv\"))\nprint(f\"Fichiers trouves : {[f.name for f in csv_files]}\")\n\n# Colonnes utiles pour limiter la memoire\nCOLS = [\n    \"id_mutation\", \"date_mutation\", \"nature_mutation\", \"valeur_fonciere\",\n    \"code_postal\", \"code_commune\", \"nom_commune\", \"code_departement\",\n    \"type_local\", \"surface_reelle_bati\", \"nombre_pieces_principales\",\n    \"surface_terrain\", \"nombre_lots\",\n    \"lot1_surface_carrez\", \"lot2_surface_carrez\",\n    \"longitude\", \"latitude\",\n]\n\nframes = []\nfor f in csv_files:\n    print(f\"Chargement de {f.name}...\")\n    tmp = pd.read_csv(f, usecols=COLS, low_memory=False)\n    frames.append(tmp)\n\ndf_raw = pd.concat(frames, ignore_index=True)\nprint(f\"\\nDataset brut : {df_raw.shape[0]:,} lignes x {df_raw.shape[1]} colonnes\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "karrycr5x4",
   "source": "# Apercu des donnees\ndf_raw.head()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "e78vunktqz7",
   "source": "# Informations generales\ndf_raw.info()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "s1zyo2r68i",
   "source": "# Statistiques descriptives\ndf_raw.describe()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "6bocktyjino",
   "source": "# Valeurs manquantes par colonne\nmissing = df_raw.isnull().sum()\nmissing_pct = (missing / len(df_raw) * 100).round(2)\npd.DataFrame({\"manquantes\": missing, \"pourcentage\": missing_pct}).sort_values(\"pourcentage\", ascending=False)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "spnilofx5bh",
   "source": "## 2. Filtrage et nettoyage\n\n- Filtrer : uniquement les **Ventes** d'**Appartements**\n- Supprimer les `valeur_fonciere` manquantes ou aberrantes (< 10 000 EUR ou > 10 000 000 EUR)\n- Supprimer les surfaces incoherentes (surface <= 0 ou > 500 m2)\n- Dedupliquer les mutations multi-lots\n- Gerer les valeurs manquantes",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "u5x5d5gc5b9",
   "source": "# 2a. Filtrer : Ventes d'Appartements uniquement\nprint(f\"Avant filtrage : {len(df_raw):,} lignes\")\nprint(f\"nature_mutation : {df_raw['nature_mutation'].value_counts().to_dict()}\")\nprint(f\"type_local      : {df_raw['type_local'].value_counts().to_dict()}\")\n\ndf = df_raw[\n    (df_raw[\"nature_mutation\"] == \"Vente\")\n    & (df_raw[\"type_local\"] == \"Appartement\")\n].copy()\nprint(f\"\\nApres filtre Vente + Appartement : {len(df):,} lignes\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "ll9xprs7g3",
   "source": "# 2b. Supprimer valeur_fonciere manquante ou aberrante\ndf = df.dropna(subset=[\"valeur_fonciere\"])\ndf = df[(df[\"valeur_fonciere\"] >= 10_000) & (df[\"valeur_fonciere\"] <= 10_000_000)]\nprint(f\"Apres filtre valeur_fonciere [10k, 10M] : {len(df):,} lignes\")\n\n# 2c. Filtrer surfaces incoherentes\ndf = df[(df[\"surface_reelle_bati\"] > 0) & (df[\"surface_reelle_bati\"] <= 500)]\nprint(f\"Apres filtre surface (0, 500] m2 : {len(df):,} lignes\")\n\n# 2d. Supprimer les doublons lies aux mutations multi-lots\n# On garde une ligne par id_mutation (celle avec la plus grande surface)\ndf = df.sort_values(\"surface_reelle_bati\", ascending=False).drop_duplicates(\n    subset=[\"id_mutation\"], keep=\"first\"\n)\nprint(f\"Apres deduplication par id_mutation : {len(df):,} lignes\")\n\n# 2e. Supprimer lignes sans coordonnees GPS\ndf = df.dropna(subset=[\"longitude\", \"latitude\"])\nprint(f\"Apres suppression sans GPS : {len(df):,} lignes\")\n\n# 2f. Remplir nombre_pieces_principales manquant par la mediane\ndf[\"nombre_pieces_principales\"] = df[\"nombre_pieces_principales\"].fillna(\n    df[\"nombre_pieces_principales\"].median()\n)\n\n# Liberer la memoire\ndel df_raw, frames\nprint(f\"\\nDataset nettoye final : {len(df):,} lignes\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "nj8c2x2bmza",
   "source": "## 3. Feature engineering\n\n- Extraction temporelle : annee, mois, trimestre\n- Prix au m2 (pour analyse exploratoire)\n- Surface Carrez : surface du lot principal si disponible\n- Target encoding du departement (prix moyen par departement)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "n2n27qlx1p",
   "source": "# 3a. Extraction temporelle\ndf[\"date_mutation\"] = pd.to_datetime(df[\"date_mutation\"])\ndf[\"annee\"] = df[\"date_mutation\"].dt.year\ndf[\"mois\"] = df[\"date_mutation\"].dt.month\ndf[\"trimestre\"] = df[\"date_mutation\"].dt.quarter\n\n# 3b. Prix au m2 (indicateur exploratoire)\ndf[\"prix_m2\"] = df[\"valeur_fonciere\"] / df[\"surface_reelle_bati\"]\n\n# 3c. Surface Carrez du lot principal (remplir par 0 si absente)\ndf[\"surface_carrez\"] = df[\"lot1_surface_carrez\"].fillna(0)\n\n# 3d. Target encoding du departement\ndept_mean = df.groupby(\"code_departement\")[\"valeur_fonciere\"].mean()\ndf[\"dept_prix_moyen\"] = df[\"code_departement\"].map(dept_mean)\n\nprint(\"Nouvelles colonnes :\", [\"annee\", \"mois\", \"trimestre\", \"prix_m2\", \"surface_carrez\", \"dept_prix_moyen\"])\ndf[[\"date_mutation\", \"annee\", \"mois\", \"trimestre\", \"prix_m2\", \"surface_carrez\", \"dept_prix_moyen\"]].head()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "a848atwuwfe",
   "source": "## 4. Analyse exploratoire (EDA)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "bonpfzabng9",
   "source": "# Distribution de la variable cible\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\naxes[0].hist(df[\"valeur_fonciere\"], bins=100, edgecolor=\"black\", alpha=0.7)\naxes[0].set_title(\"Distribution de valeur_fonciere\")\naxes[0].set_xlabel(\"Prix (EUR)\")\naxes[0].set_ylabel(\"Frequence\")\n\naxes[1].hist(np.log1p(df[\"valeur_fonciere\"]), bins=100, edgecolor=\"black\", alpha=0.7, color=\"orange\")\naxes[1].set_title(\"Distribution de log(valeur_fonciere)\")\naxes[1].set_xlabel(\"log(Prix)\")\naxes[1].set_ylabel(\"Frequence\")\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "8hj927f8ktl",
   "source": "# Prix au m2 par departement — Top 15\ntop_dept = (\n    df.groupby(\"code_departement\")[\"prix_m2\"]\n    .median()\n    .sort_values(ascending=False)\n    .head(15)\n)\n\nfig, ax = plt.subplots(figsize=(12, 5))\ntop_dept.plot(kind=\"bar\", ax=ax, color=\"steelblue\")\nax.set_title(\"Prix median au m2 par departement (Top 15)\")\nax.set_xlabel(\"Departement\")\nax.set_ylabel(\"Prix median au m2 (EUR)\")\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "87zmzmtnfn7",
   "source": "# Prix median par nombre de pieces\nfig, ax = plt.subplots(figsize=(10, 5))\ndf.groupby(\"nombre_pieces_principales\")[\"valeur_fonciere\"].median().loc[1:7].plot(\n    kind=\"bar\", ax=ax, color=\"coral\"\n)\nax.set_title(\"Prix median par nombre de pieces principales\")\nax.set_xlabel(\"Nombre de pieces\")\nax.set_ylabel(\"Prix median (EUR)\")\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "he5drdkmrd",
   "source": "# Evolution du prix median par trimestre\nprix_trim = df.groupby([\"annee\", \"trimestre\"])[\"valeur_fonciere\"].median().reset_index()\nprix_trim[\"periode\"] = prix_trim[\"annee\"].astype(str) + \"-T\" + prix_trim[\"trimestre\"].astype(str)\n\nfig, ax = plt.subplots(figsize=(14, 5))\nax.plot(prix_trim[\"periode\"], prix_trim[\"valeur_fonciere\"], marker=\"o\", linewidth=2)\nax.set_title(\"Evolution du prix median des appartements par trimestre\")\nax.set_xlabel(\"Trimestre\")\nax.set_ylabel(\"Prix median (EUR)\")\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "dqy42xejxxl",
   "source": "# Matrice de correlation des variables numeriques\nnum_cols = [\n    \"valeur_fonciere\", \"surface_reelle_bati\", \"nombre_pieces_principales\",\n    \"nombre_lots\", \"surface_carrez\", \"longitude\", \"latitude\", \"dept_prix_moyen\",\n]\ncorr = df[num_cols].corr()\n\nfig, ax = plt.subplots(figsize=(10, 8))\nsns.heatmap(corr, annot=True, fmt=\".2f\", cmap=\"RdBu_r\", center=0, ax=ax)\nax.set_title(\"Matrice de correlation\")\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "07gcq4x4nud6",
   "source": "# Scatter : surface vs prix\nfig, ax = plt.subplots(figsize=(10, 6))\nsample = df.sample(min(50_000, len(df)), random_state=42)\nax.scatter(sample[\"surface_reelle_bati\"], sample[\"valeur_fonciere\"], alpha=0.1, s=5)\nax.set_title(\"Surface reelle bati vs Valeur fonciere\")\nax.set_xlabel(\"Surface (m2)\")\nax.set_ylabel(\"Prix (EUR)\")\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "y65vjfohyh",
   "source": "## 5. Preparation des donnees pour la modelisation",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "7x086h0pmvf",
   "source": "# Features selectionnees pour le modele\nFEATURES = [\n    \"surface_reelle_bati\",\n    \"nombre_pieces_principales\",\n    \"nombre_lots\",\n    \"surface_carrez\",\n    \"longitude\",\n    \"latitude\",\n    \"annee\",\n    \"mois\",\n    \"trimestre\",\n    \"dept_prix_moyen\",\n]\nTARGET = \"valeur_fonciere\"\n\nX = df[FEATURES].copy()\ny = df[TARGET].copy()\n\nprint(f\"X shape : {X.shape}\")\nprint(f\"y shape : {y.shape}\")\nprint(f\"\\nValeurs manquantes dans X :\\n{X.isnull().sum()}\")\n\n# Separation train / test (80% / 20%)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\nprint(f\"\\nTrain : {X_train.shape[0]:,} lignes\")\nprint(f\"Test  : {X_test.shape[0]:,} lignes\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "on4dwd4s9rr",
   "source": "## 6. Modelisation\n\nModeles entraines :\n1. **Regression lineaire** (baseline)\n2. **Ridge** (regularisation L2)\n3. **Lasso** (regularisation L1)\n4. **Random Forest** (robuste aux outliers)\n5. **HistGradientBoosting** (equivalent LightGBM integre a scikit-learn)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "o6sebo2yopq",
   "source": "def evaluate_model(name, model, X_tr, y_tr, X_te, y_te):\n    \"\"\"Entraine un modele, calcule les metriques et retourne un dict de resultats.\"\"\"\n    model.fit(X_tr, y_tr)\n    y_pred = model.predict(X_te)\n\n    mae = mean_absolute_error(y_te, y_pred)\n    rmse = np.sqrt(mean_squared_error(y_te, y_pred))\n    r2 = r2_score(y_te, y_pred)\n\n    print(f\"--- {name} ---\")\n    print(f\"  MAE  : {mae:,.0f} EUR\")\n    print(f\"  RMSE : {rmse:,.0f} EUR\")\n    print(f\"  R2   : {r2:.4f}\")\n    print()\n\n    return {\"modele\": name, \"MAE\": mae, \"RMSE\": rmse, \"R2\": r2, \"model_obj\": model}",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "hgrel2yytc8",
   "source": "# Definir les modeles\nmodels = {\n    \"Regression Lineaire\": LinearRegression(),\n    \"Ridge\": Ridge(alpha=1.0),\n    \"Lasso\": Lasso(alpha=1.0, max_iter=5000),\n    \"Random Forest\": RandomForestRegressor(\n        n_estimators=200, max_depth=20, min_samples_leaf=10,\n        n_jobs=-1, random_state=42,\n    ),\n    \"HistGradientBoosting\": HistGradientBoostingRegressor(\n        max_iter=500, max_depth=10, learning_rate=0.1,\n        min_samples_leaf=20, random_state=42,\n    ),\n}\n\n# Entrainer et evaluer chaque modele\nresults = []\nfor name, model in models.items():\n    res = evaluate_model(name, model, X_train, y_train, X_test, y_test)\n    results.append(res)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "2iey6pzvurj",
   "source": "## 7. Evaluation et comparaison des modeles",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "7z7ajw9lrdi",
   "source": "# Tableau recapitulatif\ndf_results = pd.DataFrame(results)[[\"modele\", \"MAE\", \"RMSE\", \"R2\"]]\ndf_results_display = df_results.copy()\ndf_results_display[\"MAE\"] = df_results_display[\"MAE\"].map(lambda x: f\"{x:,.0f}\")\ndf_results_display[\"RMSE\"] = df_results_display[\"RMSE\"].map(lambda x: f\"{x:,.0f}\")\ndf_results_display[\"R2\"] = df_results_display[\"R2\"].map(lambda x: f\"{x:.4f}\")\ndf_results_display",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "y5rli7u261d",
   "source": "# Comparaison visuelle des metriques\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\naxes[0].barh(df_results[\"modele\"], df_results[\"R2\"], color=\"steelblue\")\naxes[0].set_title(\"R2 (plus haut = mieux)\")\naxes[0].set_xlim(0, 1)\n\naxes[1].barh(df_results[\"modele\"], df_results[\"MAE\"], color=\"coral\")\naxes[1].set_title(\"MAE en EUR (plus bas = mieux)\")\n\naxes[2].barh(df_results[\"modele\"], df_results[\"RMSE\"], color=\"mediumpurple\")\naxes[2].set_title(\"RMSE en EUR (plus bas = mieux)\")\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "dyvku238znk",
   "source": "# Validation croisee sur le meilleur modele\nbest_result = max(results, key=lambda r: r[\"R2\"])\nprint(f\"Meilleur modele : {best_result['modele']}\")\n\nbest_model = best_result[\"model_obj\"]\n\n# Cross-validation 5-fold (sur un echantillon pour la vitesse si > 200k lignes)\nif len(X) > 200_000:\n    X_cv = X.sample(200_000, random_state=42)\n    y_cv = y.loc[X_cv.index]\nelse:\n    X_cv, y_cv = X, y\n\ncv_scores = cross_val_score(\n    best_model.__class__(**best_model.get_params()),\n    X_cv, y_cv,\n    cv=5, scoring=\"r2\", n_jobs=-1,\n)\nprint(f\"\\nCross-validation 5-fold R2 : {cv_scores}\")\nprint(f\"R2 moyen : {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "ubqh91pwml",
   "source": "# Predictions vs valeurs reelles (meilleur modele)\ny_pred_best = best_model.predict(X_test)\n\nfig, ax = plt.subplots(figsize=(8, 8))\nax.scatter(y_test, y_pred_best, alpha=0.05, s=3)\nlims = [0, y_test.quantile(0.99)]\nax.plot(lims, lims, \"r--\", linewidth=2, label=\"Prediction parfaite\")\nax.set_xlim(lims)\nax.set_ylim(lims)\nax.set_xlabel(\"Valeur reelle (EUR)\")\nax.set_ylabel(\"Valeur predite (EUR)\")\nax.set_title(f\"Predictions vs Realite — {best_result['modele']}\")\nax.legend()\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "zyzdr5we7p",
   "source": "# Distribution des erreurs de prediction\nerrors = y_test - y_pred_best\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\naxes[0].hist(errors, bins=100, edgecolor=\"black\", alpha=0.7)\naxes[0].axvline(0, color=\"red\", linestyle=\"--\")\naxes[0].set_title(\"Distribution des erreurs (Reel - Predit)\")\naxes[0].set_xlabel(\"Erreur (EUR)\")\naxes[0].set_ylabel(\"Frequence\")\n\npct_error = (errors / y_test * 100).clip(-100, 100)\naxes[1].hist(pct_error, bins=100, edgecolor=\"black\", alpha=0.7, color=\"orange\")\naxes[1].axvline(0, color=\"red\", linestyle=\"--\")\naxes[1].set_title(\"Distribution des erreurs en %\")\naxes[1].set_xlabel(\"Erreur (%)\")\naxes[1].set_ylabel(\"Frequence\")\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"Erreur mediane absolue : {np.median(np.abs(errors)):,.0f} EUR\")\nprint(f\"Erreur mediane en %    : {np.median(np.abs(pct_error)):.1f} %\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "gaxtfoxv3fj",
   "source": "## 8. Importance des features",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "bvrk8chpvx",
   "source": "# Importance des features via permutation importance\nperm_imp = permutation_importance(\n    best_model, X_test, y_test,\n    n_repeats=5, random_state=42, n_jobs=-1,\n)\n\nimp_df = pd.DataFrame({\n    \"feature\": FEATURES,\n    \"importance_mean\": perm_imp.importances_mean,\n    \"importance_std\": perm_imp.importances_std,\n}).sort_values(\"importance_mean\", ascending=True)\n\nfig, ax = plt.subplots(figsize=(10, 6))\nax.barh(imp_df[\"feature\"], imp_df[\"importance_mean\"], xerr=imp_df[\"importance_std\"], color=\"steelblue\")\nax.set_title(f\"Importance des features — {best_result['modele']}\")\nax.set_xlabel(\"Diminution du R2 quand la feature est permutee\")\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "ouv6i3ua5le",
   "source": "## 9. Conclusion\n\n**Resume du pipeline** :\n1. Chargement de 6 annees de donnees DVF (2020-2025)\n2. Filtrage sur les ventes d'appartements, nettoyage des valeurs aberrantes et deduplication\n3. Feature engineering : extraction temporelle, surface Carrez, target encoding du departement\n4. Entrainement de 5 modeles : Regression Lineaire, Ridge, Lasso, Random Forest, HistGradientBoosting\n5. Evaluation via MAE, RMSE, R2 et validation croisee 5-fold\n\n**Observations** :\n- La **localisation** (longitude, latitude, prix moyen du departement) et la **surface** sont les features les plus importantes\n- Le modele **HistGradientBoosting** offre generalement les meilleures performances sur ce type de donnees tabulaires\n- Les modeles lineaires servent de baseline utile mais capturent moins bien les non-linearites du marche immobilier\n\n**Pistes d'amelioration** :\n- Enrichir avec des donnees externes (revenus par commune, proximite transports, etc.)\n- Target encoding plus sophistique (avec regularisation / cross-validation)\n- Optimisation des hyperparametres (GridSearch / Optuna)\n- Stacking de modeles",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.14.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}